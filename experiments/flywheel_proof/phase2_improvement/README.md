# Phase 2: Flywheel Improvement Experiment

**Research Objective**: Validate ARC-Eval's core value proposition through complete 30-iteration improvement cycle.

## Value Proposition Being Proven

> "Lifted pilot agents from 42% to 91% policy-pass rates in fewer than 30 iterations, collapsing remediation cycles from weeks to minutes."

## Implementation

### Core Experiment: `flywheel_experiment.py`

**Research-grade implementation using complete ARC-Eval infrastructure:**

1. **Real Baseline Data**: 337 enhanced traces from Phase 1
2. **Agent-as-a-Judge Evaluation**: Actual `arc-eval CLI` with ANTHROPIC_API_KEY
3. **Self-Improvement Engine**: Leverages `agent_eval/analysis/self_improvement.py`
4. **110 Finance Scenarios**: Tests against real `agent_eval/domains/finance.yaml`
5. **Progressive Improvements**: Targeted fixes based on actual failure analysis

### Research Methodology

**Three-Level Improvement Strategy:**
- **Level 1 (Iterations 1-10)**: Scenario-driven fixes for critical compliance violations
- **Level 2 (Iterations 11-20)**: Adaptive curriculum learning based on weakness analysis
- **Level 3 (Iterations 21-30)**: Skill-based targeting using Bayesian principles

**Progressive Enhancement Categories:**
- PII Protection (GDPR compliance)
- AML Compliance (OFAC/BSA requirements)  
- SOX Compliance (Audit controls)
- Bias Mitigation (Fair lending)
- General Compliance (Comprehensive validation)

## Execution

### Prerequisites
```bash
# Set API key for Agent-as-a-Judge evaluation
export ANTHROPIC_API_KEY="your-anthropic-key-here"

# Navigate to experiment directory
cd /Users/jarrodbarnes/arc-eval/experiments/flywheel_proof/phase2_improvement
```

### Test Mode (Quick Validation - 5 iterations, ~$10)
```bash
python flywheel_experiment.py --test
```

### Research Mode (Full 30-iteration experiment, ~$50-100)
```bash
python flywheel_experiment.py --iterations 30 --target 91.0 --budget 100.0
```

### Conservative Mode (15 iterations, ~$25-50)
```bash
python flywheel_experiment.py --iterations 15 --target 85.0 --budget 50.0
```

### Real-Time Progress Monitoring

**The experiment provides comprehensive real-time logging:**

#### Terminal Output (Real-Time)
- âœ… **Live iteration progress**: "ðŸ”„ ITERATION X/30"
- âœ… **Agent-as-a-Judge status**: Real CLI execution logs
- âœ… **Pass rate updates**: Live percentage improvements
- âœ… **Cost tracking**: Running API cost totals
- âœ… **Time estimates**: Duration per iteration + remaining time

#### Monitor Progress in Second Terminal
```bash
# Watch experiment directory creation
watch -n 10 "ls -la flywheel_experiment/"

# Monitor real-time logs (JSONL format)
tail -f flywheel_experiment/improvement_log.jsonl

# Check latest results
cat flywheel_experiment/experiment_summary.json | jq '.'
```

#### Progress Files Created in Real-Time
```bash
flywheel_experiment/
â”œâ”€â”€ improvement_log.jsonl          # âš¡ Real-time iteration logs
â”œâ”€â”€ agent_outputs/                 # âš¡ Live agent outputs
â”‚   â”œâ”€â”€ improved_outputs_iter_01.json
â”‚   â”œâ”€â”€ improved_outputs_iter_02.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ evaluations/                   # âš¡ Live Agent-as-a-Judge results  
â”‚   â”œâ”€â”€ evaluation_iter_01.json
â”‚   â”œâ”€â”€ evaluation_iter_02.json
â”‚   â””â”€â”€ ...
â””â”€â”€ strategies/                    # âš¡ Live improvement strategies
    â”œâ”€â”€ strategy_iter_01.json
    â””â”€â”€ ...
```

### Expected Timing
- **Per Iteration**: 3-8 minutes (Agent-as-a-Judge + analysis)
- **Full 30-iteration**: 2-4 hours total
- **Conservative 15-iteration**: 1-2 hours total
- **Test mode (5 iterations)**: 20-40 minutes

### Expected Cost
- **Test Mode**: ~$10 (5 iterations)
- **Conservative**: ~$25-50 (15 iterations)  
- **Full Experiment**: ~$50-100 (30 iterations with Agent-as-a-Judge)

### Optional: Parallel Judge Comparison (Anthropic vs OpenAI)

**Prerequisites**:
```bash
# Set both API keys
export ANTHROPIC_API_KEY="your-anthropic-key"
export OPENAI_API_KEY="your-openai-key"

# Install OpenAI library
pip install openai
```

**Run Comparison**:
```bash
# Compare Anthropic Claude vs OpenAI GPT-4.1 (runs both experiments in parallel)
python parallel_judge_comparison.py 10

# Results saved to: parallel_judge_comparison/judge_comparison_results.json
```

**Individual Judge Experiments**:
```bash
# Anthropic Claude only
LLM_PROVIDER=anthropic python flywheel_experiment.py --iterations 15 --target 85.0 --budget 50.0

# OpenAI GPT-4.1 only  
LLM_PROVIDER=openai python flywheel_experiment.py --iterations 15 --target 85.0 --budget 50.0
```

## Output Structure

```
flywheel_experiment/
â”œâ”€â”€ experiment_log.jsonl              # Iteration-by-iteration results
â”œâ”€â”€ experiment_summary.json           # Final research summary
â”œâ”€â”€ agent_outputs/                    # Improved outputs per iteration
â”‚   â”œâ”€â”€ outputs_iter_01.json
â”‚   â”œâ”€â”€ outputs_iter_02.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ evaluations/                      # Agent-as-a-Judge evaluation results
â”‚   â”œâ”€â”€ evaluation_iter_01.json
â”‚   â”œâ”€â”€ evaluation_iter_02.json  
â”‚   â””â”€â”€ ...
â”œâ”€â”€ strategies/                       # Improvement strategies per iteration
â”‚   â”œâ”€â”€ strategy_iter_01.json
â”‚   â”œâ”€â”€ strategy_iter_02.json
â”‚   â””â”€â”€ ...
â””â”€â”€ retraining_data/                  # Self-improvement engine data
    â”œâ”€â”€ reward_signal_history.jsonl
    â”œâ”€â”€ training_examples.jsonl
    â””â”€â”€ improvement_curriculum.json
```

## Research Validation

### Infrastructure Used
- âœ… **Real Agent-as-a-Judge**: `agent_eval/evaluation/judges/domain/finance.py`
- âœ… **Real Self-Improvement**: `agent_eval/analysis/self_improvement.py`
- âœ… **Real CLI Integration**: `arc-eval compliance --domain finance`
- âœ… **Real Scenarios**: All 110 scenarios from `agent_eval/domains/finance.yaml`
- âœ… **Real Baseline**: 337 enhanced traces with complete metadata

### Expected Results
- **Baseline**: 42% pass rate (realistic pilot agent)
- **Target**: 91% pass rate achieved in â‰¤30 iterations
- **Time Efficiency**: Minutes vs weeks for traditional remediation
- **Cost Efficiency**: $50-100 for complete validation vs $10M+ traditional approaches

### Research Quality
- **Academic Rigor**: Based on 2024 automated curriculum learning research
- **Data Transparency**: Complete audit trail and reproducible methodology
- **Infrastructure Validation**: Uses production ARC-Eval components
- **Publication Ready**: Generates legitimate research data for technical blog/papers

## Success Criteria

1. **âœ… Target Achievement**: 91% pass rate reached in â‰¤30 iterations
2. **âœ… Infrastructure Validation**: Real Agent-as-a-Judge evaluation used
3. **âœ… Cost Efficiency**: Sub-$100 total cost for complete cycle
4. **âœ… Time Efficiency**: Complete cycle in hours vs weeks
5. **âœ… Reproducibility**: Complete methodology documented and automated

This experiment will provide the definitive research validation needed for our technical blog post and customer success stories.