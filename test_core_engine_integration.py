#!/usr/bin/env python3
"""
Test script for core engine judge integration.

This script tests the new judge-enhanced core engine without requiring API keys.
It demonstrates the hybrid evaluation approach with graceful fallback.
"""

import json
import sys
from pathlib import Path

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent))

from agent_eval.core.engine import EvaluationEngine
from agent_eval.core.types import AgentOutput


def test_core_engine_integration():
    """Test the core engine with judge integration."""
    print("ğŸ” Testing Core Engine Judge Integration")
    print("=" * 60)
    
    # Load test data
    test_file = Path("examples/quickstart/finance_example.json")
    if not test_file.exists():
        print(f"âŒ Test file not found: {test_file}")
        return False
    
    with open(test_file, 'r') as f:
        test_data = json.load(f)
    
    print(f"ğŸ“ Loaded {len(test_data)} test outputs from {test_file}")
    
    # Initialize engine
    try:
        engine = EvaluationEngine(domain="finance")
        print(f"âœ… Engine initialized for domain: finance")
        print(f"ğŸ“Š Available scenarios: {len(engine.eval_pack.scenarios)}")
    except Exception as e:
        print(f"âŒ Engine initialization failed: {e}")
        return False
    
    # Test evaluation with judge integration
    try:
        print("\nğŸš€ Running evaluation with judge integration...")
        results = engine.evaluate(test_data)
        
        print(f"âœ… Evaluation completed successfully!")
        print(f"ğŸ“Š Results: {len(results)} scenarios evaluated")
        
        # Analyze results
        passed = sum(1 for r in results if r.passed)
        failed = len(results) - passed
        judge_enhanced = sum(1 for r in results if r.has_judge_analysis())
        
        print(f"\nğŸ“ˆ Results Summary:")
        print(f"  â€¢ Passed: {passed}")
        print(f"  â€¢ Failed: {failed}")
        print(f"  â€¢ Judge Enhanced: {judge_enhanced}")
        
        # Show sample results
        print(f"\nğŸ” Sample Results:")
        for i, result in enumerate(results[:3]):
            status = "âœ… PASS" if result.passed else "âŒ FAIL"
            confidence = result.get_confidence_score()
            judge_used = "ğŸ§  Judge" if result.has_judge_analysis() else "ğŸ“‹ Rule"
            
            print(f"  {i+1}. {status} | {confidence:.1%} confidence | {judge_used} | {result.scenario_name}")
            
            if result.has_judge_analysis():
                print(f"     Judge: {result.judge_reasoning[:100]}...")
            elif result.failure_reason:
                print(f"     Rule: {result.failure_reason[:100]}...")
        
        # Test specific compliance violations
        print(f"\nğŸ” Compliance Violation Detection:")
        violations_found = []
        for result in results:
            if not result.passed and result.failure_reason:
                if any(keyword in result.failure_reason.lower() for keyword in ['pii', 'ssn', 'aml', 'kyc', 'bias']):
                    violations_found.append(result)
        
        print(f"  â€¢ Compliance violations detected: {len(violations_found)}")
        for violation in violations_found[:2]:
            print(f"    - {violation.scenario_name}: {violation.failure_reason}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_judge_triggering_logic():
    """Test the smart judge triggering logic."""
    print("\nğŸ§  Testing Judge Triggering Logic")
    print("=" * 60)
    
    # Create mock engine to test triggering logic
    engine = EvaluationEngine(domain="finance")
    
    # Test scenarios for triggering logic
    test_cases = [
        {"passed": False, "confidence": 0.9, "severity": "medium", "should_trigger": True, "reason": "Failed scenario"},
        {"passed": True, "confidence": 0.6, "severity": "medium", "should_trigger": True, "reason": "Low confidence"},
        {"passed": True, "confidence": 0.9, "severity": "critical", "should_trigger": True, "reason": "Critical severity"},
        {"passed": True, "confidence": 0.9, "severity": "low", "should_trigger": False, "reason": "High confidence pass"},
    ]
    
    from agent_eval.core.types import EvaluationResult, EvaluationScenario
    
    for i, case in enumerate(test_cases):
        # Create mock result and scenario
        scenario = EvaluationScenario(
            id=f"test_{i}",
            name=f"Test Scenario {i}",
            description="Test scenario",
            severity=case["severity"],
            test_type="negative",
            failure_indicators=[],
            expected_behavior="",
            compliance=[],
            remediation="",
            category="test",
            input_template=""
        )
        
        result = EvaluationResult(
            scenario_id=scenario.id,
            scenario_name=scenario.name,
            description=scenario.description,
            severity=scenario.severity,
            test_type=scenario.test_type,
            passed=case["passed"],
            status="passed" if case["passed"] else "failed",
            confidence=case["confidence"]
        )
        
        # Test triggering logic
        should_trigger = engine._should_trigger_judge_analysis(result, scenario)
        expected = case["should_trigger"]
        
        status = "âœ…" if should_trigger == expected else "âŒ"
        print(f"  {status} Case {i+1}: {case['reason']} -> Trigger: {should_trigger} (expected: {expected})")
    
    print("âœ… Judge triggering logic test completed")


if __name__ == "__main__":
    print("ğŸš€ Arc-Eval Core Engine Integration Test")
    print("=" * 60)
    
    success = test_core_engine_integration()
    test_judge_triggering_logic()
    
    if success:
        print("\nâœ… All tests passed! Core engine judge integration working correctly.")
        print("ğŸ¯ Key achievements:")
        print("  â€¢ Hybrid evaluation (rule-based + judge enhancement)")
        print("  â€¢ Smart judge triggering for failed scenarios")
        print("  â€¢ Graceful fallback when judges unavailable")
        print("  â€¢ Backward compatibility maintained")
        sys.exit(0)
    else:
        print("\nâŒ Tests failed! Check the output above for details.")
        sys.exit(1)
